{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-04-01 to 2025-04-30\n",
      "Months to download: ['202504']\n",
      "Will download 5 files for states: NSW1, VIC1, QLD1, SA1, TAS1\n",
      "\n",
      "Downloading and processing file 1/5: PRICE_AND_DEMAND_202504_NSW1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_NSW1.csv\n",
      "\n",
      "Downloading and processing file 2/5: PRICE_AND_DEMAND_202504_VIC1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_VIC1.csv\n",
      "\n",
      "Downloading and processing file 3/5: PRICE_AND_DEMAND_202504_QLD1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_QLD1.csv\n",
      "\n",
      "Downloading and processing file 4/5: PRICE_AND_DEMAND_202504_SA1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_SA1.csv\n",
      "\n",
      "Downloading and processing file 5/5: PRICE_AND_DEMAND_202504_TAS1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_TAS1.csv\n",
      "\n",
      "==================== DOWNLOAD SUMMARY ====================\n",
      "Total files processed: 5\n",
      "Successfully downloaded: 5\n",
      "Failed downloads: 0\n",
      "Files saved to: /Users/paj/Desktop/NEM EXCEL/\n",
      "============================================================\n",
      "\n",
      "==================== CONSOLIDATING DATA ====================\n",
      "✓ Consolidated file saved: PRICE_AND_DEMAND.csv\n",
      "\n",
      "==================== CALCULATING METRICS ====================\n",
      "Filtering data to range: 2025-04-01 to 2025-04-30\n",
      "Records before filtering: 43200\n",
      "Records after filtering: 43195\n",
      "Found data for months: 2025-04\n",
      "\n",
      "Calculating metrics for 2025-04...\n",
      "  Processing NSW1 for 2025-04...\n",
      "    NSW1: Demand=4,883,820, TWAP=$104.07, VWAP=$114.72\n",
      "      Price bands - >$5K: 0.4h, $300-5K: 5.0h, $150-300: 124.4h\n",
      "      $100-150: 204.5h, $0-100: 308.8h, <$0: 76.8h\n",
      "  Processing VIC1 for 2025-04...\n",
      "    VIC1: Demand=3,208,505, TWAP=$74.76, VWAP=$83.69\n",
      "      Price bands - >$5K: 0.0h, $300-5K: 1.1h, $150-300: 63.3h\n",
      "      $100-150: 222.0h, $0-100: 316.4h, <$0: 117.1h\n",
      "  Processing QLD1 for 2025-04...\n",
      "    QLD1: Demand=4,351,401, TWAP=$98.50, VWAP=$109.81\n",
      "      Price bands - >$5K: 0.4h, $300-5K: 4.3h, $150-300: 115.0h\n",
      "      $100-150: 185.7h, $0-100: 302.6h, <$0: 111.9h\n",
      "  Processing SA1 for 2025-04...\n",
      "    SA1: Demand=907,452, TWAP=$88.83, VWAP=$104.21\n",
      "      Price bands - >$5K: 0.0h, $300-5K: 3.8h, $150-300: 135.3h\n",
      "      $100-150: 225.5h, $0-100: 229.7h, <$0: 125.7h\n",
      "  Processing TAS1 for 2025-04...\n",
      "    TAS1: Demand=769,636, TWAP=$100.53, VWAP=$102.08\n",
      "      Price bands - >$5K: 0.0h, $300-5K: 1.0h, $150-300: 41.2h\n",
      "      $100-150: 437.2h, $0-100: 240.0h, <$0: 0.5h\n",
      "✓ Metrics file saved: PRICE_AND_DEMAND_METRICS.csv\n",
      "\n",
      "==================== METRICS SUMMARY ====================\n",
      "Full metrics table:\n",
      "STATE MONTH_YEAR     DEMAND   TWAP   VWAP  Hours >$5000  Hours $5000-$300  Hours $300-$150  Hours $150-$100  Hours $0-$100  Hours <$0\n",
      " NSW1    2025-04 4883820.09 104.07 114.72          0.42              5.00           124.42           204.50         308.75      76.83\n",
      " VIC1    2025-04 3208505.25  74.76  83.69          0.00              1.08            63.33           222.00         316.42     117.08\n",
      " QLD1    2025-04 4351401.48  98.50 109.81          0.42              4.33           115.00           185.67         302.58     111.92\n",
      "  SA1    2025-04  907451.62  88.83 104.21          0.00              3.75           135.33           225.50         229.67     125.67\n",
      " TAS1    2025-04  769635.92 100.53 102.08          0.00              1.00            41.25           437.17         240.00       0.50\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Price and Demand\n",
    "# Ouputs a download like collated from specific range from AEMO\n",
    "# Metrics file which is all calculations for Total Demand by state, TWAP, VWAP and Price Bands\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ==================== USER CONFIGURATION ====================\n",
    "\n",
    "start_month = 4\n",
    "start_year = 2025\n",
    "\n",
    "end_month = 4\n",
    "end_year = 2025\n",
    "\n",
    "# Specify which states to download data for\n",
    "# Available options: 'NSW1', 'VIC1', 'QLD1', 'SA1', 'TAS1'\n",
    "states = ['NSW1', 'VIC1', 'QLD1', 'SA1', 'TAS1']\n",
    "\n",
    "# Output file names (without .csv extension) --> Change name for adhoc analysis and don't disturb workflow\n",
    "# Default workflow name --> 'PRICE_AND_DEMAND' , 'PRICE_AND_DEMAND_METRICS'\n",
    "consolidated_filename = 'PRICE_AND_DEMAND'\n",
    "metrics_filename = 'PRICE_AND_DEMAND_METRICS'\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "# Convert user-friendly input to proper date range\n",
    "startdate = f'{start_year}-{start_month:02d}-01'\n",
    "# Get the last day of the end month automatically\n",
    "if end_month == 12:\n",
    "    next_month = 1\n",
    "    next_year = end_year + 1\n",
    "else:\n",
    "    next_month = end_month + 1\n",
    "    next_year = end_year\n",
    "\n",
    "# Calculate last day of end month\n",
    "last_day = (pd.Timestamp(f'{next_year}-{next_month:02d}-01') - pd.Timedelta(days=1)).day\n",
    "enddate = f'{end_year}-{end_month:02d}-{last_day}'\n",
    "\n",
    "print(f\"Date range: {startdate} to {enddate}\")\n",
    "\n",
    "# Generate date range\n",
    "daterange = pd.date_range(startdate, enddate, freq='MS').strftime('%Y%m').tolist()\n",
    "print(f\"Months to download: {daterange}\")\n",
    "\n",
    "# Specify the directory to store downloaded files\n",
    "directory = os.path.expanduser('~/Desktop/NEM EXCEL/')  # macOS path to your NEM EXCEL folder\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Define the root URL for dataset retrieval\n",
    "root_url = 'https://aemo.com.au/aemo/data/nem/priceanddemand/PRICE_AND_DEMAND_'\n",
    "\n",
    "# Generate URLs for all combinations of dates and states\n",
    "urls = []\n",
    "for date in daterange:\n",
    "    for state in states:\n",
    "        urls.append(root_url + date + '_' + state + '.csv')\n",
    "\n",
    "print(f\"Will download {len(urls)} files for states: {', '.join(states)}\")\n",
    "\n",
    "# Define headers to mimic browser behavior\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Referer': 'https://aemo.com.au/'\n",
    "}\n",
    "\n",
    "# Download and process files directly without saving individually\n",
    "downloaded = 0\n",
    "failed = 0\n",
    "all_data = []\n",
    "\n",
    "for i, url in enumerate(urls, 1):\n",
    "    print(f\"\\nDownloading and processing file {i}/{len(urls)}: {os.path.basename(url)}\")\n",
    "    \n",
    "    try:\n",
    "        # Create a session to maintain cookies\n",
    "        session = requests.Session()\n",
    "        \n",
    "        # Send a GET request to the URL\n",
    "        response = session.get(url, headers=headers, timeout=30, allow_redirects=True)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Read CSV data directly from response content\n",
    "            from io import StringIO\n",
    "            csv_data = StringIO(response.text)\n",
    "            df = pd.read_csv(csv_data)\n",
    "            all_data.append(df)\n",
    "            print(f\"✓ Downloaded and processed successfully: {os.path.basename(url)}\")\n",
    "            downloaded += 1\n",
    "        else:\n",
    "            print(f\"✗ Failed to download. Status code: {response.status_code}\")\n",
    "            failed += 1\n",
    "            # Try alternative URL format if available\n",
    "            if response.status_code == 403:\n",
    "                print(f\"  Access denied. This file may not be publicly available.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"✗ Error downloading: {e}\")\n",
    "        failed += 1\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"✗ Error: Empty or invalid CSV data\")\n",
    "        failed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing CSV: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    time.sleep(7)  # Increased delay to be more respectful\n",
    "\n",
    "print(f\"\\n==================== DOWNLOAD SUMMARY ====================\")\n",
    "print(f\"Total files processed: {len(urls)}\")\n",
    "print(f\"Successfully downloaded: {downloaded}\")\n",
    "print(f\"Failed downloads: {failed}\")\n",
    "print(f\"Files saved to: {directory}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==================== CONSOLIDATION AND METRICS ====================\n",
    "\n",
    "if all_data:\n",
    "    print(f\"\\n==================== CONSOLIDATING DATA ====================\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Create consolidated filename\n",
    "    consolidated_file_path = os.path.join(directory, f'{consolidated_filename}.csv')\n",
    "    combined_df.to_csv(consolidated_file_path, index=False)\n",
    "    print(f\"✓ Consolidated file saved: {consolidated_filename}.csv\")\n",
    "    \n",
    "    # ==================== CALCULATE METRICS ====================\n",
    "    print(f\"\\n==================== CALCULATING METRICS ====================\")\n",
    "    \n",
    "    # Create metrics dataframe\n",
    "    metrics_data = []\n",
    "    \n",
    "    # Determine the region column name (it might be 'REGION' or 'REGIONID')\n",
    "    region_col = None\n",
    "    if 'REGION' in combined_df.columns:\n",
    "        region_col = 'REGION'\n",
    "    elif 'REGIONID' in combined_df.columns:\n",
    "        region_col = 'REGIONID'\n",
    "    else:\n",
    "        print(\"Warning: Could not find REGION or REGIONID column\")\n",
    "        print(f\"Available columns: {list(combined_df.columns)}\")\n",
    "    \n",
    "    if region_col and 'TOTALDEMAND' in combined_df.columns and 'RRP' in combined_df.columns:\n",
    "        # Convert SETTLEMENTDATE to datetime for month extraction\n",
    "        combined_df['SETTLEMENTDATE'] = pd.to_datetime(combined_df['SETTLEMENTDATE'])\n",
    "        \n",
    "        # Filter data to only include the months we actually requested\n",
    "        start_date = pd.Timestamp(f'{start_year}-{start_month:02d}-01')\n",
    "        if end_month == 12:\n",
    "            end_date = pd.Timestamp(f'{end_year + 1}-01-01') - pd.Timedelta(seconds=1)\n",
    "        else:\n",
    "            end_date = pd.Timestamp(f'{end_year}-{end_month + 1:02d}-01') - pd.Timedelta(seconds=1)\n",
    "        \n",
    "        print(f\"Filtering data to range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # Filter the dataframe to only include requested date range\n",
    "        filtered_df = combined_df[(combined_df['SETTLEMENTDATE'] >= start_date) & \n",
    "                                 (combined_df['SETTLEMENTDATE'] <= end_date)].copy()\n",
    "        \n",
    "        print(f\"Records before filtering: {len(combined_df)}\")\n",
    "        print(f\"Records after filtering: {len(filtered_df)}\")\n",
    "        \n",
    "        # Get unique month-year combinations in the filtered data\n",
    "        filtered_df['MONTH_YEAR'] = filtered_df['SETTLEMENTDATE'].dt.strftime('%Y-%m')\n",
    "        unique_months = sorted(filtered_df['MONTH_YEAR'].unique())\n",
    "        \n",
    "        print(f\"Found data for months: {', '.join(unique_months)}\")\n",
    "        \n",
    "        # Calculate metrics for each state and each month\n",
    "        for month_year in unique_months:\n",
    "            print(f\"\\nCalculating metrics for {month_year}...\")\n",
    "            month_data = filtered_df[filtered_df['MONTH_YEAR'] == month_year]\n",
    "            \n",
    "            for state in states:\n",
    "                print(f\"  Processing {state} for {month_year}...\")\n",
    "                \n",
    "                # Filter data for current state and month\n",
    "                state_month_data = month_data[month_data[region_col] == state]\n",
    "                \n",
    "                if not state_month_data.empty:\n",
    "                    # Calculate basic metrics\n",
    "                    demand = state_month_data['TOTALDEMAND'].sum() / 12\n",
    "                    twap = state_month_data['RRP'].mean()\n",
    "                    \n",
    "                    # Calculate VWAP (Volume Weighted Average Price)\n",
    "                    total_value = (state_month_data['TOTALDEMAND'] * state_month_data['RRP']).sum()\n",
    "                    total_demand = state_month_data['TOTALDEMAND'].sum()\n",
    "                    vwap = total_value / total_demand if total_demand != 0 else 0\n",
    "                    \n",
    "                    # Calculate price band hours (equivalent to COUNTIFS in Excel, divided by 12)\n",
    "                    hours_over_5000 = len(state_month_data[state_month_data['RRP'] >= 5000]) / 12\n",
    "                    hours_300_to_5000 = len(state_month_data[(state_month_data['RRP'] >= 300) & (state_month_data['RRP'] < 5000)]) / 12\n",
    "                    hours_150_to_300 = len(state_month_data[(state_month_data['RRP'] >= 150) & (state_month_data['RRP'] < 300)]) / 12\n",
    "                    hours_100_to_150 = len(state_month_data[(state_month_data['RRP'] >= 100) & (state_month_data['RRP'] < 150)]) / 12\n",
    "                    hours_0_to_100 = len(state_month_data[(state_month_data['RRP'] >= 0) & (state_month_data['RRP'] < 100)]) / 12\n",
    "                    hours_under_0 = len(state_month_data[state_month_data['RRP'] < 0]) / 12\n",
    "                    \n",
    "                    # Add all metrics to the data list\n",
    "                    metrics_data.append({\n",
    "                        'STATE': state,\n",
    "                        'MONTH_YEAR': month_year,\n",
    "                        'DEMAND': demand,\n",
    "                        'TWAP': twap,\n",
    "                        'VWAP': vwap,\n",
    "                        'Hours >$5000': hours_over_5000,\n",
    "                        'Hours $5000-$300': hours_300_to_5000,\n",
    "                        'Hours $300-$150': hours_150_to_300,\n",
    "                        'Hours $150-$100': hours_100_to_150,\n",
    "                        'Hours $0-$100': hours_0_to_100,\n",
    "                        'Hours <$0': hours_under_0\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"    {state}: Demand={demand:,.0f}, TWAP=${twap:.2f}, VWAP=${vwap:.2f}\")\n",
    "                    print(f\"      Price bands - >$5K: {hours_over_5000:.1f}h, $300-5K: {hours_300_to_5000:.1f}h, $150-300: {hours_150_to_300:.1f}h\")\n",
    "                    print(f\"      $100-150: {hours_100_to_150:.1f}h, $0-100: {hours_0_to_100:.1f}h, <$0: {hours_under_0:.1f}h\")\n",
    "                else:\n",
    "                    print(f\"    No data found for {state} in {month_year}\")\n",
    "    \n",
    "    # Save metrics file\n",
    "    if metrics_data:\n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        # Create metrics filename\n",
    "        metrics_file_path = os.path.join(directory, f'{metrics_filename}.csv')\n",
    "        \n",
    "        metrics_df.to_csv(metrics_file_path, index=False)\n",
    "        print(f\"✓ Metrics file saved: {metrics_filename}.csv\")\n",
    "        \n",
    "        # Display summary table\n",
    "        print(f\"\\n==================== METRICS SUMMARY ====================\")\n",
    "        print(\"Full metrics table:\")\n",
    "        print(metrics_df.to_string(index=False, float_format='%.2f'))\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"No metrics calculated - check column names in your CSV files\")\n",
    "        print(f\"Available columns: {list(combined_df.columns)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid CSV files could be processed for consolidation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-04-01 to 2025-04-30\n",
      "Months to download: ['202504']\n",
      "Will download 5 files for states: NSW1, QLD1, VIC1, SA1, TAS1\n",
      "\n",
      "Downloading and processing file 1/5: PRICE_AND_DEMAND_202504_NSW1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_NSW1.csv\n",
      "\n",
      "Downloading and processing file 2/5: PRICE_AND_DEMAND_202504_QLD1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_QLD1.csv\n",
      "\n",
      "Downloading and processing file 3/5: PRICE_AND_DEMAND_202504_VIC1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_VIC1.csv\n",
      "\n",
      "Downloading and processing file 4/5: PRICE_AND_DEMAND_202504_SA1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_SA1.csv\n",
      "\n",
      "Downloading and processing file 5/5: PRICE_AND_DEMAND_202504_TAS1.csv\n",
      "✓ Downloaded and processed successfully: PRICE_AND_DEMAND_202504_TAS1.csv\n",
      "\n",
      "==================== DOWNLOAD SUMMARY ====================\n",
      "Total files processed: 5\n",
      "Successfully downloaded: 5\n",
      "Failed downloads: 0\n",
      "Files saved to: /Users/paj/Desktop/NEM EXCEL/\n",
      "============================================================\n",
      "\n",
      "==================== CONSOLIDATING DATA ====================\n",
      "✓ Data consolidated in memory (not saving consolidated file)\n",
      "Filtering data to range: 2025-04-01 to 2025-04-30\n",
      "Records before filtering: 43200\n",
      "Records after filtering: 43195\n",
      "Found data for months: 2025-04\n",
      "\n",
      "==================== HOURLY PRICE BAND ANALYSIS ====================\n",
      "Analyzing hourly patterns for period: Apr 2025\n",
      "Processing hour 00:00...\n",
      "  Hour 00: <0=8,410, 0-100=215,696, 100-300=362,456, 300-1000=0, 1000+=0\n",
      "Processing hour 01:00...\n",
      "  Hour 01: <0=9,275, 0-100=273,653, 100-300=281,938, 300-1000=0, 1000+=0\n",
      "Processing hour 02:00...\n",
      "  Hour 02: <0=13,513, 0-100=288,227, 100-300=242,972, 300-1000=121, 1000+=0\n",
      "Processing hour 03:00...\n",
      "  Hour 03: <0=14,656, 0-100=293,759, 100-300=228,256, 300-1000=0, 1000+=0\n",
      "Processing hour 04:00...\n",
      "  Hour 04: <0=5,740, 0-100=307,034, 100-300=228,481, 300-1000=91, 1000+=0\n",
      "Processing hour 05:00...\n",
      "  Hour 05: <0=4,987, 0-100=239,246, 100-300=320,342, 300-1000=117, 1000+=0\n",
      "Processing hour 06:00...\n",
      "  Hour 06: <0=7,533, 0-100=147,758, 100-300=447,999, 300-1000=1,294, 1000+=0\n",
      "Processing hour 07:00...\n",
      "  Hour 07: <0=80,072, 0-100=326,648, 100-300=199,806, 300-1000=3,036, 1000+=0\n",
      "Processing hour 08:00...\n",
      "  Hour 08: <0=167,902, 0-100=305,055, 100-300=96,010, 300-1000=337, 1000+=0\n",
      "Processing hour 09:00...\n",
      "  Hour 09: <0=190,613, 0-100=293,835, 100-300=32,865, 300-1000=701, 1000+=0\n",
      "Processing hour 10:00...\n",
      "  Hour 10: <0=226,684, 0-100=236,378, 100-300=20,574, 300-1000=96, 1000+=0\n",
      "Processing hour 11:00...\n",
      "  Hour 11: <0=215,314, 0-100=238,399, 100-300=14,710, 300-1000=0, 1000+=0\n",
      "Processing hour 12:00...\n",
      "  Hour 12: <0=207,432, 0-100=242,717, 100-300=15,557, 300-1000=253, 1000+=0\n",
      "Processing hour 13:00...\n",
      "  Hour 13: <0=185,065, 0-100=281,220, 100-300=17,088, 300-1000=88, 1000+=0\n",
      "Processing hour 14:00...\n",
      "  Hour 14: <0=128,109, 0-100=370,380, 100-300=24,664, 300-1000=0, 1000+=0\n",
      "Processing hour 15:00...\n",
      "  Hour 15: <0=66,055, 0-100=456,604, 100-300=65,802, 300-1000=269, 1000+=0\n",
      "Processing hour 16:00...\n",
      "  Hour 16: <0=31,494, 0-100=271,508, 100-300=363,617, 300-1000=147, 1000+=0\n",
      "Processing hour 17:00...\n",
      "  Hour 17: <0=13,249, 0-100=53,155, 100-300=602,480, 300-1000=43,212, 1000+=18,659\n",
      "Processing hour 18:00...\n",
      "  Hour 18: <0=1,907, 0-100=45,519, 100-300=676,173, 300-1000=22,708, 1000+=4,334\n",
      "Processing hour 19:00...\n",
      "  Hour 19: <0=0, 0-100=87,175, 100-300=633,397, 300-1000=0, 1000+=1,327\n",
      "Processing hour 20:00...\n",
      "  Hour 20: <0=0, 0-100=101,502, 100-300=589,875, 300-1000=0, 1000+=0\n",
      "Processing hour 21:00...\n",
      "  Hour 21: <0=0, 0-100=134,963, 100-300=525,608, 300-1000=0, 1000+=0\n",
      "Processing hour 22:00...\n",
      "  Hour 22: <0=2,347, 0-100=135,923, 100-300=493,470, 300-1000=139, 1000+=0\n",
      "Processing hour 23:00...\n",
      "  Hour 23: <0=2,875, 0-100=175,381, 100-300=434,782, 300-1000=0, 1000+=0\n",
      "✓ Hourly analysis file saved: Price_Band.csv\n",
      "\n",
      "==================== HOURLY ANALYSIS SUMMARY ====================\n",
      "Hourly price band demand totals for period: Apr 2025\n",
      " HOUR INCLUDED_RANGE     <0  0-100  100-300  300-1000  1000+\n",
      "    0       Apr 2025   8410 215696   362456         0      0\n",
      "    1       Apr 2025   9275 273653   281938         0      0\n",
      "    2       Apr 2025  13513 288227   242972       121      0\n",
      "    3       Apr 2025  14656 293759   228256         0      0\n",
      "    4       Apr 2025   5740 307034   228481        91      0\n",
      "    5       Apr 2025   4987 239246   320342       117      0\n",
      "    6       Apr 2025   7533 147758   447999      1294      0\n",
      "    7       Apr 2025  80072 326648   199806      3036      0\n",
      "    8       Apr 2025 167902 305055    96010       337      0\n",
      "    9       Apr 2025 190613 293835    32865       701      0\n",
      "   10       Apr 2025 226684 236378    20574        96      0\n",
      "   11       Apr 2025 215314 238399    14710         0      0\n",
      "   12       Apr 2025 207432 242717    15557       253      0\n",
      "   13       Apr 2025 185065 281220    17088        88      0\n",
      "   14       Apr 2025 128109 370380    24664         0      0\n",
      "   15       Apr 2025  66055 456604    65802       269      0\n",
      "   16       Apr 2025  31494 271508   363617       147      0\n",
      "   17       Apr 2025  13249  53155   602480     43212  18659\n",
      "   18       Apr 2025   1907  45519   676173     22708   4334\n",
      "   19       Apr 2025      0  87175   633397         0   1327\n",
      "   20       Apr 2025      0 101502   589875         0      0\n",
      "   21       Apr 2025      0 134963   525608         0      0\n",
      "   22       Apr 2025   2347 135923   493470       139      0\n",
      "   23       Apr 2025   2875 175381   434782         0      0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Price Band Analysis\n",
    "# For STATE break-down by Demand (0,100,300 etc) just do manually caz python cannot make an easier toggle + dynamic chart\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ==================== USER CONFIGURATION ====================\n",
    "\n",
    "# Output file names (without .csv extension) --> Change name for adhoc analysis and don't disturb workflow\n",
    "# Default workflow name --> 'Price_Band'\n",
    "consolidated_filename = 'Price_Band'\n",
    "\n",
    "start_month = 4\n",
    "start_year = 2025\n",
    "\n",
    "end_month = 4\n",
    "end_year = 2025\n",
    "\n",
    "# Specify which states to download data for\n",
    "# Available options: 'NSW1', 'QLD1', 'VIC1', 'SA1', 'TAS1'\n",
    "states = ['NSW1', 'QLD1', 'VIC1', 'SA1', 'TAS1']\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "# Convert user-friendly input to proper date range\n",
    "startdate = f'{start_year}-{start_month:02d}-01'\n",
    "# Get the last day of the end month automatically\n",
    "if end_month == 12:\n",
    "    next_month = 1\n",
    "    next_year = end_year + 1\n",
    "else:\n",
    "    next_month = end_month + 1\n",
    "    next_year = end_year\n",
    "\n",
    "# Calculate last day of end month\n",
    "last_day = (pd.Timestamp(f'{next_year}-{next_month:02d}-01') - pd.Timedelta(days=1)).day\n",
    "enddate = f'{end_year}-{end_month:02d}-{last_day}'\n",
    "\n",
    "print(f\"Date range: {startdate} to {enddate}\")\n",
    "\n",
    "# Generate date range\n",
    "daterange = pd.date_range(startdate, enddate, freq='MS').strftime('%Y%m').tolist()\n",
    "print(f\"Months to download: {daterange}\")\n",
    "\n",
    "# Specify the directory to store downloaded files\n",
    "directory = os.path.expanduser('~/Desktop/NEM EXCEL/')  # macOS path to your NEM EXCEL folder\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Define the root URL for dataset retrieval\n",
    "root_url = 'https://aemo.com.au/aemo/data/nem/priceanddemand/PRICE_AND_DEMAND_'\n",
    "\n",
    "# Generate URLs for all combinations of dates and states\n",
    "urls = []\n",
    "for date in daterange:\n",
    "    for state in states:\n",
    "        urls.append(root_url + date + '_' + state + '.csv')\n",
    "\n",
    "print(f\"Will download {len(urls)} files for states: {', '.join(states)}\")\n",
    "\n",
    "# Define headers to mimic browser behavior\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Referer': 'https://aemo.com.au/'\n",
    "}\n",
    "\n",
    "# Download and process files directly without saving individually\n",
    "downloaded = 0\n",
    "failed = 0\n",
    "all_data = []\n",
    "\n",
    "for i, url in enumerate(urls, 1):\n",
    "    print(f\"\\nDownloading and processing file {i}/{len(urls)}: {os.path.basename(url)}\")\n",
    "    \n",
    "    try:\n",
    "        # Create a session to maintain cookies\n",
    "        session = requests.Session()\n",
    "        \n",
    "        # Send a GET request to the URL\n",
    "        response = session.get(url, headers=headers, timeout=30, allow_redirects=True)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Read CSV data directly from response content\n",
    "            from io import StringIO\n",
    "            csv_data = StringIO(response.text)\n",
    "            df = pd.read_csv(csv_data)\n",
    "            all_data.append(df)\n",
    "            print(f\"✓ Downloaded and processed successfully: {os.path.basename(url)}\")\n",
    "            downloaded += 1\n",
    "        else:\n",
    "            print(f\"✗ Failed to download. Status code: {response.status_code}\")\n",
    "            failed += 1\n",
    "            # Try alternative URL format if available\n",
    "            if response.status_code == 403:\n",
    "                print(f\"  Access denied. This file may not be publicly available.\")\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"✗ Error downloading: {e}\")\n",
    "        failed += 1\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"✗ Error: Empty or invalid CSV data\")\n",
    "        failed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing CSV: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    time.sleep(7)  # Increased delay to be more respectful\n",
    "\n",
    "print(f\"\\n==================== DOWNLOAD SUMMARY ====================\")\n",
    "print(f\"Total files processed: {len(urls)}\")\n",
    "print(f\"Successfully downloaded: {downloaded}\")\n",
    "print(f\"Failed downloads: {failed}\")\n",
    "print(f\"Files saved to: {directory}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==================== CONSOLIDATION AND METRICS ====================\n",
    "\n",
    "if all_data:\n",
    "    print(f\"\\n==================== CONSOLIDATING DATA ====================\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # REMOVED: Create consolidated filename - no longer saving this file\n",
    "    print(f\"✓ Data consolidated in memory (not saving consolidated file)\")\n",
    "    \n",
    "    # REMOVED: CALCULATE METRICS section - no longer calculating state metrics\n",
    "    \n",
    "    # Determine the region column name (it might be 'REGION' or 'REGIONID')\n",
    "    region_col = None\n",
    "    if 'REGION' in combined_df.columns:\n",
    "        region_col = 'REGION'\n",
    "    elif 'REGIONID' in combined_df.columns:\n",
    "        region_col = 'REGIONID'\n",
    "    else:\n",
    "        print(\"Warning: Could not find REGION or REGIONID column\")\n",
    "        print(f\"Available columns: {list(combined_df.columns)}\")\n",
    "    \n",
    "    if region_col and 'TOTALDEMAND' in combined_df.columns and 'RRP' in combined_df.columns:\n",
    "        # Convert SETTLEMENTDATE to datetime for month extraction\n",
    "        combined_df['SETTLEMENTDATE'] = pd.to_datetime(combined_df['SETTLEMENTDATE'])\n",
    "        \n",
    "        # Filter data to only include the months we actually requested\n",
    "        start_date = pd.Timestamp(f'{start_year}-{start_month:02d}-01')\n",
    "        if end_month == 12:\n",
    "            end_date = pd.Timestamp(f'{end_year + 1}-01-01') - pd.Timedelta(seconds=1)\n",
    "        else:\n",
    "            end_date = pd.Timestamp(f'{end_year}-{end_month + 1:02d}-01') - pd.Timedelta(seconds=1)\n",
    "        \n",
    "        print(f\"Filtering data to range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # Filter the dataframe to only include requested date range\n",
    "        filtered_df = combined_df[(combined_df['SETTLEMENTDATE'] >= start_date) & \n",
    "                                 (combined_df['SETTLEMENTDATE'] <= end_date)].copy()\n",
    "        \n",
    "        print(f\"Records before filtering: {len(combined_df)}\")\n",
    "        print(f\"Records after filtering: {len(filtered_df)}\")\n",
    "        \n",
    "        # Get unique month-year combinations in the filtered data\n",
    "        filtered_df['MONTH_YEAR'] = filtered_df['SETTLEMENTDATE'].dt.strftime('%Y-%m')\n",
    "        unique_months = sorted(filtered_df['MONTH_YEAR'].unique())\n",
    "        \n",
    "        print(f\"Found data for months: {', '.join(unique_months)}\")\n",
    "\n",
    "    # ==================== HOURLY PRICE BAND ANALYSIS ====================\n",
    "    print(f\"\\n==================== HOURLY PRICE BAND ANALYSIS ====================\")\n",
    "    \n",
    "    if region_col and 'TOTALDEMAND' in combined_df.columns and 'RRP' in combined_df.columns:\n",
    "        # Use the same filtered data\n",
    "        if 'filtered_df' in locals():\n",
    "            # Extract hour from SETTLEMENTDATE\n",
    "            filtered_df['HOUR'] = filtered_df['SETTLEMENTDATE'].dt.hour\n",
    "            \n",
    "            # Create included range description\n",
    "            if len(unique_months) == 1:\n",
    "                # Single month: \"Mar 2025\"\n",
    "                month_obj = pd.to_datetime(unique_months[0] + '-01')\n",
    "                included_range = month_obj.strftime('%b %Y')\n",
    "            else:\n",
    "                # Multiple months: \"Feb 2025 - Apr 2025\"\n",
    "                start_month_obj = pd.to_datetime(unique_months[0] + '-01')\n",
    "                end_month_obj = pd.to_datetime(unique_months[-1] + '-01')\n",
    "                included_range = f\"{start_month_obj.strftime('%b %Y')} - {end_month_obj.strftime('%b %Y')}\"\n",
    "            \n",
    "            print(f\"Analyzing hourly patterns for period: {included_range}\")\n",
    "            \n",
    "            hourly_data = []\n",
    "            \n",
    "            # Process each hour (0-23)\n",
    "            for hour in range(24):\n",
    "                print(f\"Processing hour {hour:02d}:00...\")\n",
    "                \n",
    "                # Filter data for current hour across ALL months in the analysis period\n",
    "                hour_data = filtered_df[filtered_df['HOUR'] == hour]\n",
    "                \n",
    "                if not hour_data.empty:\n",
    "                    # Step 1: Calculate cumulative totals for each threshold (like your Table 1)\n",
    "                    # This matches: SUMIF($C:$C, $A:$A, STATE, $F:$F, $H20, $D:$D, \"<=\"&I18)/12\n",
    "                    \n",
    "                    cumulative_totals = {}\n",
    "                    thresholds = [0, 100, 300, 1000, float('inf')]  # 0, 100, 300, 1000, Above\n",
    "                    \n",
    "                    for threshold in thresholds:\n",
    "                        total_for_threshold = 0\n",
    "                        \n",
    "                        # Calculate for each state separately (like your Table 1)\n",
    "                        for state in states:\n",
    "                            state_hour_data = hour_data[hour_data[region_col] == state]\n",
    "                            if not state_hour_data.empty:\n",
    "                                if threshold == float('inf'):\n",
    "                                    # For \"Above\" - get all data\n",
    "                                    state_total = state_hour_data['TOTALDEMAND'].sum() / 12\n",
    "                                else:\n",
    "                                    # For specific thresholds - get data <= threshold\n",
    "                                    state_total = state_hour_data[state_hour_data['RRP'] <= threshold]['TOTALDEMAND'].sum() / 12\n",
    "                                \n",
    "                                total_for_threshold += state_total\n",
    "                        \n",
    "                        cumulative_totals[threshold] = total_for_threshold\n",
    "                    \n",
    "                    # Step 2: Calculate differences to get the actual bands (like your Table 3)\n",
    "                    total_under_0 = cumulative_totals[0]  # Everything <= 0\n",
    "                    total_0_to_100 = cumulative_totals[100] - cumulative_totals[0]  # (<=100) - (<=0)\n",
    "                    total_100_to_300 = cumulative_totals[300] - cumulative_totals[100]  # (<=300) - (<=100)\n",
    "                    total_300_to_1000 = cumulative_totals[1000] - cumulative_totals[300]  # (<=1000) - (<=300)\n",
    "                    total_1000_plus = cumulative_totals[float('inf')] - cumulative_totals[1000]  # (All) - (<=1000)\n",
    "                    \n",
    "                    hourly_data.append({\n",
    "                        'HOUR': hour,\n",
    "                        'INCLUDED_RANGE': included_range,\n",
    "                        '<0': total_under_0,\n",
    "                        '0-100': total_0_to_100,\n",
    "                        '100-300': total_100_to_300,\n",
    "                        '300-1000': total_300_to_1000,\n",
    "                        '1000+': total_1000_plus\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  Hour {hour:02d}: <0={total_under_0:,.0f}, 0-100={total_0_to_100:,.0f}, 100-300={total_100_to_300:,.0f}, 300-1000={total_300_to_1000:,.0f}, 1000+={total_1000_plus:,.0f}\")\n",
    "                else:\n",
    "                    # Fill with zeros if no data for this hour\n",
    "                    hourly_data.append({\n",
    "                        'HOUR': hour,\n",
    "                        'INCLUDED_RANGE': included_range,\n",
    "                        '<0': 0,\n",
    "                        '0-100': 0,\n",
    "                        '100-300': 0,\n",
    "                        '300-1000': 0,\n",
    "                        '1000+': 0\n",
    "                    })\n",
    "                    print(f\"  Hour {hour:02d}: No data - filled with zeros\")\n",
    "            \n",
    "            # Save hourly analysis file with configurable name\n",
    "            if hourly_data:\n",
    "                hourly_df = pd.DataFrame(hourly_data)\n",
    "                \n",
    "                # Create hourly filename using configurable name\n",
    "                hourly_file_path = os.path.join(directory, f'{consolidated_filename}.csv')\n",
    "                \n",
    "                hourly_df.to_csv(hourly_file_path, index=False)\n",
    "                print(f\"✓ Hourly analysis file saved: {consolidated_filename}.csv\")\n",
    "                \n",
    "                # Display hourly summary table\n",
    "                print(f\"\\n==================== HOURLY ANALYSIS SUMMARY ====================\")\n",
    "                print(f\"Hourly price band demand totals for period: {included_range}\")\n",
    "                print(hourly_df.to_string(index=False, float_format='%.0f'))\n",
    "                print(\"=\"*60)\n",
    "            else:\n",
    "                print(\"No hourly data calculated\")\n",
    "        else:\n",
    "            print(\"No filtered data available for hourly analysis\")\n",
    "    else:\n",
    "        print(\"Required columns not found for hourly analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"No valid CSV files could be processed for consolidation\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f79994913ccb4c81c421a51466dbe971696a8bea0839e8a0b59247fbe24294"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
