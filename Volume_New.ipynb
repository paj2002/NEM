{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading NEM reference data...\n",
      "✅ Loaded 528 DUIDs from NEM reference data\n",
      "📅 Total date range: 2014-07-01 to 2025-05-31 (3987 days)\n",
      "🔄 Breaking into 11 periods due to API 365-day limit:\n",
      "   Period 1: 2014-07-01 to 2015-06-30\n",
      "   Period 2: 2015-07-01 to 2016-06-30\n",
      "   Period 3: 2016-07-01 to 2017-06-30\n",
      "   Period 4: 2017-07-01 to 2018-06-30\n",
      "   Period 5: 2018-07-01 to 2019-06-30\n",
      "   Period 6: 2019-07-01 to 2020-06-30\n",
      "   Period 7: 2020-07-01 to 2021-06-30\n",
      "   Period 8: 2021-07-01 to 2022-06-30\n",
      "   Period 9: 2022-07-01 to 2023-06-30\n",
      "   Period 10: 2023-07-01 to 2024-06-30\n",
      "   Period 11: 2024-07-01 to 2025-05-31\n",
      "📁 Output filename: NSW_Volume.csv\n",
      "📁 Decommissioned filename: NSW_Decommissioned_Volume.csv\n",
      "⚡ Values will be in MWh (energy units)\n",
      "🏞️ Region filter active: NSW1\n",
      "🔄 Fetching facility codes...\n",
      "✅ Retrieved 508 facilities\n",
      "🔄 Fetching energy data...\n",
      "📦 Fetching batch 1 of 26\n",
      "📦 Fetching batch 2 of 26\n",
      "📦 Fetching batch 3 of 26\n",
      "📦 Fetching batch 4 of 26\n",
      "📦 Fetching batch 5 of 26\n",
      "📦 Fetching batch 6 of 26\n",
      "📦 Fetching batch 7 of 26\n",
      "📦 Fetching batch 8 of 26\n",
      "📦 Fetching batch 9 of 26\n",
      "📦 Fetching batch 10 of 26\n",
      "📦 Fetching batch 11 of 26\n",
      "📦 Fetching batch 12 of 26\n",
      "📦 Fetching batch 13 of 26\n",
      "📦 Fetching batch 14 of 26\n",
      "📦 Fetching batch 15 of 26\n",
      "📦 Fetching batch 16 of 26\n",
      "📦 Fetching batch 17 of 26\n",
      "📦 Fetching batch 18 of 26\n",
      "📦 Fetching batch 19 of 26\n",
      "📦 Fetching batch 20 of 26\n",
      "📦 Fetching batch 21 of 26\n",
      "📦 Fetching batch 22 of 26\n",
      "📦 Fetching batch 23 of 26\n",
      "📦 Fetching batch 24 of 26\n",
      "📦 Fetching batch 25 of 26\n",
      "📦 Fetching batch 26 of 26\n",
      "✅ Retrieved 13464 data points\n",
      "\n",
      "🔄 Fetching Period 2: 2015-07-01 to 2016-06-30\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 2: 14213 data points, 261 DUIDs\n",
      "\n",
      "🔄 Fetching Period 3: 2016-07-01 to 2017-06-30\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 3: 14924 data points, 251 DUIDs\n",
      "\n",
      "🔄 Fetching Period 4: 2017-07-01 to 2018-06-30\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 4: 16339 data points, 267 DUIDs\n",
      "\n",
      "🔄 Fetching Period 5: 2018-07-01 to 2019-06-30\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 5: 18887 data points, 299 DUIDs\n",
      "\n",
      "🔄 Fetching Period 6: 2019-07-01 to 2020-06-30\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 6: 20404 data points, 321 DUIDs\n",
      "\n",
      "🔄 Fetching Period 7: 2020-07-01 to 2021-06-30\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 7: 24048 data points, 355 DUIDs\n",
      "\n",
      "🔄 Fetching Period 8: 2021-07-01 to 2022-06-30\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 8: 27787 data points, 387 DUIDs\n",
      "\n",
      "🔄 Fetching Period 9: 2022-07-01 to 2023-06-30\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 9: 29170 data points, 416 DUIDs\n",
      "\n",
      "🔄 Fetching Period 10: 2023-07-01 to 2024-06-30\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 10: 31958 data points, 431 DUIDs\n",
      "\n",
      "🔄 Fetching Period 11: 2024-07-01 to 2025-05-31\n",
      "   📦 Batch 1/26\n",
      "   📦 Batch 2/26\n",
      "   📦 Batch 3/26\n",
      "   📦 Batch 4/26\n",
      "   📦 Batch 5/26\n",
      "   📦 Batch 6/26\n",
      "   📦 Batch 7/26\n",
      "   📦 Batch 8/26\n",
      "   📦 Batch 9/26\n",
      "   📦 Batch 10/26\n",
      "   📦 Batch 11/26\n",
      "   📦 Batch 12/26\n",
      "   📦 Batch 13/26\n",
      "   📦 Batch 14/26\n",
      "   📦 Batch 15/26\n",
      "   📦 Batch 16/26\n",
      "   📦 Batch 17/26\n",
      "   📦 Batch 18/26\n",
      "   📦 Batch 19/26\n",
      "   📦 Batch 20/26\n",
      "   📦 Batch 21/26\n",
      "   📦 Batch 22/26\n",
      "   📦 Batch 23/26\n",
      "   📦 Batch 24/26\n",
      "   📦 Batch 25/26\n",
      "   📦 Batch 26/26\n",
      "   ✅ Period 11: 31214 data points, 465 DUIDs\n",
      "\n",
      "✅ Total data retrieved: 242408 records across 11 periods\n",
      "\n",
      "📊 DUID CATEGORIZATION:\n",
      "   • Reference DUIDs (from latest period 11): 465\n",
      "   • Decommissioned DUIDs (historical only): 43\n",
      "🔄 Aggregating data by month...\n",
      "📊 Region breakdown:\n",
      "   • NSW1: 97 DUIDs\n",
      "🔋 Storage facilities in output: 93\n",
      "✅ Enhanced file saved: NSW_Volume.csv\n",
      "✅ Decommissioned file saved: NSW_Decommissioned_Volume.csv (1 DUIDs)\n",
      "\n",
      "📊 SUMMARY REPORT:\n",
      "   • Files saved as: NSW_Volume.csv, NSW_Decommissioned_Volume.csv\n",
      "   • Values in MWh (energy units)\n",
      "   • Date range processed: 2014-07-01 to 2025-05-31\n",
      "   • Periods processed: 11/11\n",
      "   • Region filter: NSW1\n",
      "   • Reference DUIDs (main file): 465\n",
      "   • Decommissioned DUIDs: 43\n",
      "   • Total unique DUIDs: 508\n",
      "   • NEM reference data loaded: 528 DUIDs\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Open Electricity Data Puller with NEM Data Integration\n",
    "# Will need the DUID excel from AEMO Generation Information EXCEL\n",
    "# WITH MULTI-YEAR SUPPORT: Auto-handles API 365-day limit + Decommissioned facility tracking\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import openpyxl\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ==================== USER CONFIGURATION ====================\n",
    "\n",
    "# Output file names (without .csv extension) --> Change name for adhoc analysis and don't disturb workflow\n",
    "# Default workflow name --> 'ALLSTATES_Volume'\n",
    "consolidated_filename = 'ALLSTATES_Volume'\n",
    "decommissioned_filename = 'ALLSTATES_Decommissioned_Volume'  # NEW: For historical DUIDs\n",
    "\n",
    "# API Key (get from https://platform.openelectricity.org.au)\n",
    "API_KEY = \"oe_3ZbuDQVhMCk1guoQqd7eBcWi\"\n",
    "\n",
    "# 🌐 Network code (market you want data from)\n",
    "# - \"NEM\" → National Electricity Market (eastern Australia)\n",
    "# - \"WEM\" → Western Australia\n",
    "# - \"AEMO_ROOFTOP\" → Rooftop PV estimates\n",
    "# - \"APVI\" → Community PV data\n",
    "NETWORK_CODE = \"NEM\"\n",
    "\n",
    "# 🏞️ REGION FILTER - Filter by specific regions/states\n",
    "REGION_FILTER = [\"NSW1\", \"VIC1\", \"QLD1\", \"SA1\", \"TAS1\"]\n",
    "# REGION_FILTER = [\"NSW1\", \"VIC1\", \"QLD1\", \"SA1\", \"TAS1\"]  # All states\n",
    "\n",
    "# 📅 Time interval \n",
    "# Options:\n",
    "# - \"1h\" → Hourly\n",
    "# - \"1d\" → Daily\n",
    "# - \"7d\" → Weekly\n",
    "# - \"1M\" → Monthly\n",
    "# - \"3M\" → Quarterly\n",
    "# - \"season\" → Seasonal\n",
    "# - \"1y\" → Calendar year\n",
    "# - \"fy\" → Financial year\n",
    "INTERVAL = \"1d\"\n",
    "\n",
    "# Metric (you can only choose ONE per request)\n",
    "#\"energy\" → MWh (electricity generated/consumed)-> Volume tab in Excel\n",
    "#\"power\" → MW (average power/generation) -> feeds in price later anyways\n",
    "#\"market_value\" → $AUD (total market value/revenue)-> Revenue tab in Excel\n",
    "#\"emissions\" → tCO2e (carbon emissions)\n",
    "#\"renewable_proportion\" → % (share of renewables)\n",
    "METRIC = \"energy\"\n",
    "\n",
    "# 🆕 NEW: RETRY CONFIGURATION\n",
    "MAX_RETRIES = 3  # Number of retries for failed requests\n",
    "RETRY_DELAY = 5  # Base delay between retries (seconds)\n",
    "BATCH_DELAY = 1.0  # Delay between batches (seconds) - increased for stability\n",
    "\n",
    "# ==================== ENHANCED DATE CONFIGURATION ====================\n",
    "# Specify the date range - just month and year!\n",
    "# 🆕 NEW: Code will automatically loop if date range > 365 days (API limit)\n",
    "start_month = 4    \n",
    "start_year = 2025\n",
    "\n",
    "end_month = 4      \n",
    "end_year = 2025\n",
    "# ===========================================================\n",
    "\n",
    "def create_robust_session():\n",
    "    \"\"\"\n",
    "    🆕 NEW: Create a requests session with retry strategy and SSL handling\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Define retry strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    \n",
    "    # Mount adapter with retry strategy\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    \n",
    "    # Set timeout\n",
    "    session.timeout = 30\n",
    "    \n",
    "    return session\n",
    "\n",
    "def calculate_date_periods(start_month, start_year, end_month, end_year):\n",
    "    \"\"\"\n",
    "    🆕 NEW FUNCTION: Calculate if we need multiple API calls and break into periods\n",
    "    API has 365-day limit, so we break long ranges into yearly chunks\n",
    "    \"\"\"\n",
    "    # Convert user-friendly input to proper date range\n",
    "    startdate = f'{start_year}-{start_month:02d}-01'\n",
    "    # Get the last day of the end month automatically\n",
    "    if end_month == 12:\n",
    "        next_month = 1\n",
    "        next_year = end_year + 1\n",
    "    else:\n",
    "        next_month = end_month + 1\n",
    "        next_year = end_year\n",
    "\n",
    "    # Calculate last day of end month\n",
    "    last_day = (pd.Timestamp(f'{next_year}-{next_month:02d}-01') - pd.Timedelta(days=1)).day\n",
    "    enddate = f'{end_year}-{end_month:02d}-{last_day}'\n",
    "    \n",
    "    # Create start and end dates\n",
    "    start_date = datetime.strptime(startdate, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(enddate, \"%Y-%m-%d\")\n",
    "    \n",
    "    # Check if range > 365 days\n",
    "    total_days = (end_date - start_date).days\n",
    "    print(f\"📅 Total date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')} ({total_days} days)\")\n",
    "    \n",
    "    if total_days <= 365:\n",
    "        # Single period - works exactly like before\n",
    "        print(f\"📅 Date range: {startdate} to {enddate}\")\n",
    "        return [(start_date, end_date)]\n",
    "    else:\n",
    "        # Multiple periods needed due to API limit\n",
    "        periods = []\n",
    "        current_start = start_date\n",
    "        \n",
    "        while current_start < end_date:\n",
    "            # Calculate end of current period (1 year from start)\n",
    "            current_end = current_start + relativedelta(years=1) - timedelta(days=1)\n",
    "            \n",
    "            # Don't go past the final end date\n",
    "            if current_end > end_date:\n",
    "                current_end = end_date\n",
    "                \n",
    "            periods.append((current_start, current_end))\n",
    "            \n",
    "            # Next period starts the day after current period ends\n",
    "            current_start = current_end + timedelta(days=1)\n",
    "        \n",
    "        print(f\"🔄 Breaking into {len(periods)} periods due to API 365-day limit:\")\n",
    "        for i, (start, end) in enumerate(periods, 1):\n",
    "            print(f\"   Period {i}: {start.strftime('%Y-%m-%d')} to {end.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        return periods\n",
    "\n",
    "# === FUNCTION: Load NEM reference data ===\n",
    "def load_nem_reference_data(file_path=\"NEM DATA.xlsx\"):\n",
    "    \"\"\"\n",
    "    Load the NEM reference data and create a DUID lookup dictionary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        nem_df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
    "        \n",
    "        # Filter out records without DUID\n",
    "        nem_df_clean = nem_df[nem_df['DUID'].notna()].copy()\n",
    "        \n",
    "        # Create lookup dictionary\n",
    "        duid_lookup = {}\n",
    "        for _, row in nem_df_clean.iterrows():\n",
    "            duid = row['DUID']\n",
    "            duid_lookup[duid] = {\n",
    "                'Region': row.get('Region', 'N/A'),\n",
    "                'Facility': row.get('Facility', 'N/A'),\n",
    "                'Owner': row.get('Owner', 'N/A'),\n",
    "                'Number_of_Units': row.get('Number of Units', 'N/A'),\n",
    "                'Nameplate_Capacity_MW': row.get('Nameplate Capacity (MW)', 'N/A'),\n",
    "                'Storage_Capacity_MWh': row.get('Storage Capacity (MWh)', 'N/A'),\n",
    "                'Expected_Closure_Year': row.get('Expected Closure Year', 'N/A'),\n",
    "                'Fueltech': row.get('Fueltech', 'N/A')\n",
    "            }\n",
    "        \n",
    "        print(f\"✅ Loaded {len(duid_lookup)} DUIDs from NEM reference data\")\n",
    "        return duid_lookup\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠️ NEM DATA.xlsx not found. Proceeding without reference data.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading NEM reference data: {e}\")\n",
    "        return {}\n",
    "\n",
    "# === FUNCTION: Get all facilities from network ===\n",
    "def fetch_all_facility_codes(api_key, network_code=\"NEM\"):\n",
    "    \"\"\"Get all facilities from network with retry logic\"\"\"\n",
    "    session = create_robust_session()\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            url = \"https://api.openelectricity.org.au/v4/facilities/\"\n",
    "            headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "            params = {\"network_id\": network_code, \"with_clerk\": \"true\"}\n",
    "            \n",
    "            response = session.get(url, headers=headers, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                facilities = [f[\"code\"] for f in response.json().get(\"data\", []) if len(f[\"code\"]) < 30]\n",
    "                session.close()\n",
    "                return facilities\n",
    "            else:\n",
    "                print(f\"❌ Error fetching facilities: {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY * (attempt + 1))\n",
    "    \n",
    "    session.close()\n",
    "    print(\"❌ Failed to fetch facilities after all retries\")\n",
    "    return []\n",
    "\n",
    "# === FUNCTION: Check if DUID should be included based on region filter ===\n",
    "def should_include_duid(duid, metadata, duid_lookup, region_filter):\n",
    "    \"\"\"\n",
    "    Check if a DUID should be included based on the region filter\n",
    "    \"\"\"\n",
    "    if not region_filter:  # No filter, include all\n",
    "        return True\n",
    "    \n",
    "    # Get region from NEM data first (more accurate), fallback to API data\n",
    "    region = None\n",
    "    if duid in duid_lookup:\n",
    "        region = duid_lookup[duid].get('Region', 'N/A')\n",
    "    \n",
    "    if region == 'N/A' and duid in metadata:\n",
    "        region = metadata[duid].get('Region', 'N/A')\n",
    "    \n",
    "    return region in region_filter\n",
    "\n",
    "# === FUNCTION: Fetch data for all facilities in batches ===\n",
    "def fetch_data_for_period(facility_codes, metric, duid_lookup, start_date, end_date, period_num):\n",
    "    \"\"\"\n",
    "    🆕 ENHANCED: Fetch data for a single time period with robust error handling\n",
    "    \"\"\"\n",
    "    if period_num == 1:\n",
    "        print(\"🔄 Fetching energy data...\")\n",
    "    else:\n",
    "        print(f\"\\n🔄 Fetching Period {period_num}: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    all_records = []\n",
    "    all_metadata = {}\n",
    "    \n",
    "    # Convert dates to API format\n",
    "    DATE_START = start_date.strftime(\"%Y-%m-%dT00:00:00\")\n",
    "    DATE_END = end_date.strftime(\"%Y-%m-%dT00:00:00\")\n",
    "\n",
    "    session = create_robust_session()\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    base_url = f\"https://api.openelectricity.org.au/v4/data/facilities/{NETWORK_CODE}\"\n",
    "    BATCH_SIZE = 20\n",
    "\n",
    "    total_batches = len(facility_codes) // BATCH_SIZE + 1\n",
    "    successful_batches = 0\n",
    "    failed_batches = 0\n",
    "\n",
    "    for i in range(0, len(facility_codes), BATCH_SIZE):\n",
    "        batch = facility_codes[i:i + BATCH_SIZE]\n",
    "        batch_num = i // BATCH_SIZE + 1\n",
    "        \n",
    "        params = {\n",
    "            \"facility_code\": batch,\n",
    "            \"metrics\": [metric],\n",
    "            \"interval\": INTERVAL,\n",
    "            \"date_start\": DATE_START,\n",
    "            \"date_end\": DATE_END,\n",
    "            \"with_clerk\": \"true\"\n",
    "        }\n",
    "\n",
    "        # 🆕 RETRY LOGIC FOR EACH BATCH\n",
    "        batch_success = False\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                if period_num == 1:\n",
    "                    print(f\"📦 Fetching batch {batch_num} of {total_batches}\")\n",
    "                else:\n",
    "                    print(f\"   📦 Batch {batch_num}/{total_batches}\")\n",
    "                \n",
    "                response = session.get(base_url, headers=headers, params=params, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    \n",
    "                    # Process the data (same logic as before)\n",
    "                    for facility_block in data.get(\"data\", []):\n",
    "                        facility_code = facility_block.get(\"facility_code\", \"N/A\")\n",
    "                        facility_region = facility_block.get(\"network_region\", \"N/A\")\n",
    "                        facility_fueltech = facility_block.get(\"fueltech_id\", \"N/A\")\n",
    "\n",
    "                        for result in facility_block.get(\"results\", []):\n",
    "                            duid = result[\"columns\"].get(\"unit_code\", \"N/A\")\n",
    "                            name = result.get(\"name\", duid)\n",
    "                            key = duid\n",
    "\n",
    "                            if duid == \"N/A\":\n",
    "                                continue\n",
    "\n",
    "                            # Enhanced metadata with NEM reference data\n",
    "                            base_metadata = {\n",
    "                                \"DUID\": duid,\n",
    "                                \"Name\": name,\n",
    "                                \"Facility\": facility_code,\n",
    "                                \"Region\": facility_region,\n",
    "                                \"Fueltech\": facility_fueltech\n",
    "                            }\n",
    "                            \n",
    "                            if duid in duid_lookup:\n",
    "                                nem_data = duid_lookup[duid]\n",
    "                                enhanced_metadata = {\n",
    "                                    \"DUID\": duid,\n",
    "                                    \"Name\": name,\n",
    "                                    \"Facility\": nem_data.get('Facility', facility_code),\n",
    "                                    \"Region\": nem_data.get('Region', facility_region),\n",
    "                                    \"Fueltech\": nem_data.get('Fueltech', facility_fueltech),\n",
    "                                    \"Owner\": nem_data.get('Owner', 'N/A'),\n",
    "                                    \"Number_of_Units\": nem_data.get('Number_of_Units', 'N/A'),\n",
    "                                    \"Nameplate_Capacity_MW\": nem_data.get('Nameplate_Capacity_MW', 'N/A'),\n",
    "                                    \"Storage_Capacity_MWh\": nem_data.get('Storage_Capacity_MWh', 'N/A'),  # ✅ FIXED: Correct key name\n",
    "                                    \"Expected_Closure_Year\": nem_data.get('Expected_Closure_Year', 'N/A')\n",
    "                                }\n",
    "                                all_metadata[key] = enhanced_metadata\n",
    "                            else:\n",
    "                                enhanced_metadata = base_metadata.copy()\n",
    "                                enhanced_metadata.update({\n",
    "                                    \"Owner\": 'N/A',\n",
    "                                    \"Number_of_Units\": 'N/A',\n",
    "                                    \"Nameplate_Capacity_MW\": 'N/A',\n",
    "                                    \"Storage_Capacity_MWh\": 'N/A',\n",
    "                                    \"Expected_Closure_Year\": 'N/A'\n",
    "                                })\n",
    "                                all_metadata[key] = enhanced_metadata\n",
    "\n",
    "                            if not should_include_duid(duid, all_metadata, duid_lookup, REGION_FILTER):\n",
    "                                continue\n",
    "\n",
    "                            # Process numerical data - NO DIVISION for energy data (it's already in MWh)\n",
    "                            for timestamp, value in result.get(\"data\", []):\n",
    "                                all_records.append({\n",
    "                                    \"timestamp\": timestamp[:10],  # Extract date part only\n",
    "                                    \"key\": key,\n",
    "                                    \"value\": value if value is not None else 0,  # Keep original MWh values\n",
    "                                    \"period\": period_num  # 🆕 NEW: Track which period this data came from\n",
    "                                })\n",
    "                    \n",
    "                    successful_batches += 1\n",
    "                    batch_success = True\n",
    "                    break  # Success, exit retry loop\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"   ❌ HTTP {response.status_code}: {response.text[:100]}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Batch {batch_num} attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    print(f\"   🔄 Retrying in {RETRY_DELAY * (attempt + 1)} seconds...\")\n",
    "                    time.sleep(RETRY_DELAY * (attempt + 1))\n",
    "\n",
    "        if not batch_success:\n",
    "            failed_batches += 1\n",
    "            print(f\"   ❌ Batch {batch_num} failed after all retries - continuing with next batch\")\n",
    "\n",
    "        # Friendly pause between batches\n",
    "        time.sleep(BATCH_DELAY)\n",
    "\n",
    "    session.close()\n",
    "    \n",
    "    if period_num == 1:\n",
    "        print(f\"✅ Retrieved {len(all_records)} data points\")\n",
    "    else:\n",
    "        print(f\"   ✅ Period {period_num}: {len(all_records)} data points, {len(all_metadata)} DUIDs\")\n",
    "    \n",
    "    if failed_batches > 0:\n",
    "        print(f\"   ⚠️ Note: {failed_batches} batches failed but continuing with available data\")\n",
    "    \n",
    "    return all_records, all_metadata\n",
    "\n",
    "def categorize_duids(all_periods_metadata):\n",
    "    \"\"\"\n",
    "    🆕 NEW FUNCTION: Categorize DUIDs into reference (latest period) vs decommissioned\n",
    "    Reference DUIDs = present in latest period (used for main file structure)\n",
    "    Decommissioned DUIDs = present in historical periods but NOT in latest period\n",
    "    \"\"\"\n",
    "    # Get DUIDs from each period\n",
    "    period_duids = {}\n",
    "    for period_num, metadata in all_periods_metadata.items():\n",
    "        period_duids[period_num] = set(metadata.keys())\n",
    "    \n",
    "    # Latest period DUIDs = reference\n",
    "    latest_period = max(period_duids.keys())\n",
    "    reference_duids = period_duids[latest_period]\n",
    "    \n",
    "    # Decommissioned = in historical periods but NOT in latest\n",
    "    all_historical_duids = set()\n",
    "    for period_num, duids in period_duids.items():\n",
    "        if period_num < latest_period:\n",
    "            all_historical_duids.update(duids)\n",
    "    \n",
    "    decommissioned_duids = all_historical_duids - reference_duids\n",
    "    \n",
    "    print(f\"\\n📊 DUID CATEGORIZATION:\")\n",
    "    print(f\"   • Reference DUIDs (from latest period {latest_period}): {len(reference_duids)}\")\n",
    "    print(f\"   • Decommissioned DUIDs (historical only): {len(decommissioned_duids)}\")\n",
    "    \n",
    "    return reference_duids, decommissioned_duids, latest_period\n",
    "\n",
    "def create_output_files(all_records, all_periods_metadata, reference_duids, decommissioned_duids, latest_period):\n",
    "    \"\"\"\n",
    "    ✅ FIXED: Create two output files - now includes ALL decommissioned DUIDs\n",
    "    Main file: Reference DUIDs with full time series\n",
    "    Decommissioned file: ALL historical DUIDs (even with minimal data)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if not df.empty:\n",
    "        # 🧮 Add 'month' for grouping (same as your original code)\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])  \n",
    "        df[\"month\"] = df[\"timestamp\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "        # 🗃️ Pivot into matrix format - aggregating by month (same as your original code)\n",
    "        print(\"🔄 Aggregating data by month...\")\n",
    "        monthly_df = df.groupby([\"month\", \"key\"])[\"value\"].sum().unstack(fill_value=0)\n",
    "        \n",
    "        # Get all unique months for consistent time series\n",
    "        all_months = sorted(df[\"month\"].unique())\n",
    "    else:\n",
    "        print(\"⚠️ No data to aggregate\")\n",
    "        return\n",
    "    \n",
    "    # === MAIN FILE: Reference DUIDs ===\n",
    "    reference_columns = [col for col in monthly_df.columns if col in reference_duids]\n",
    "    main_df = monthly_df[reference_columns]\n",
    "    \n",
    "    # Use latest period metadata for headers\n",
    "    latest_metadata = all_periods_metadata[latest_period]\n",
    "    \n",
    "    # Show region breakdown\n",
    "    if reference_columns:\n",
    "        region_counts = {}\n",
    "        storage_count = 0\n",
    "        for col in reference_columns:\n",
    "            if col in latest_metadata:\n",
    "                region = latest_metadata[col].get('Region', 'Unknown')\n",
    "                region_counts[region] = region_counts.get(region, 0) + 1\n",
    "                \n",
    "                # Count storage facilities\n",
    "                storage_capacity = latest_metadata[col].get('Storage_Capacity_MWh', 'N/A')\n",
    "                if storage_capacity != 'N/A' and storage_capacity != '' and storage_capacity != 0:\n",
    "                    storage_count += 1\n",
    "        \n",
    "        print(f\"📊 Region breakdown:\")\n",
    "        for region, count in sorted(region_counts.items()):\n",
    "            print(f\"   • {region}: {count} DUIDs\")\n",
    "        \n",
    "        print(f\"🔋 Storage facilities in output: {storage_count}\")\n",
    "    \n",
    "    # 🏷️ Add enhanced metadata as header rows\n",
    "    has_enhanced_data = any('Owner' in meta for meta in latest_metadata.values())\n",
    "\n",
    "    if has_enhanced_data:\n",
    "        meta_fields = [\"DUID\", \"Name\", \"Facility\", \"Region\", \"Fueltech\", \n",
    "                       \"Owner\", \"Number_of_Units\", \"Nameplate_Capacity_MW\", \n",
    "                       \"Storage_Capacity_MWh\", \"Expected_Closure_Year\"]\n",
    "    else:\n",
    "        meta_fields = [\"DUID\", \"Name\", \"Facility\", \"Region\", \"Fueltech\"]\n",
    "\n",
    "    main_meta_rows = []\n",
    "    for field in meta_fields:\n",
    "        row = {}\n",
    "        for col in main_df.columns:\n",
    "            if col in latest_metadata:\n",
    "                value = latest_metadata[col].get(field, \"N/A\")\n",
    "                if pd.isna(value):\n",
    "                    value = \"N/A\"\n",
    "                row[col] = value\n",
    "            else:\n",
    "                row[col] = \"N/A\"\n",
    "        main_meta_rows.append(row)\n",
    "\n",
    "    main_meta_df = pd.DataFrame(main_meta_rows, index=meta_fields)\n",
    "    main_separator = pd.DataFrame(index=[\"---\"], columns=main_df.columns)\n",
    "    main_final_df = pd.concat([main_meta_df, main_separator, main_df])\n",
    "    \n",
    "    # 💾 Save file with configurable name\n",
    "    main_filename = f\"{consolidated_filename}.csv\"\n",
    "    main_final_df.to_csv(main_filename)\n",
    "    print(f\"✅ Enhanced file saved: {main_filename}\")\n",
    "    \n",
    "    # === ✅ FIXED DECOMMISSIONED FILE: Include ALL Historical DUIDs ===\n",
    "    if decommissioned_duids:\n",
    "        print(f\"🔄 Creating decommissioned file with ALL {len(decommissioned_duids)} historical DUIDs...\")\n",
    "        \n",
    "        # Get columns that exist in monthly_df\n",
    "        existing_decomm_columns = [col for col in monthly_df.columns if col in decommissioned_duids]\n",
    "        \n",
    "        # Get DUIDs that were identified as decommissioned but don't have data in monthly_df  \n",
    "        missing_decomm_duids = decommissioned_duids - set(existing_decomm_columns)\n",
    "        \n",
    "        print(f\"   • DUIDs with data: {len(existing_decomm_columns)}\")\n",
    "        print(f\"   • DUIDs with minimal/no data: {len(missing_decomm_duids)}\")\n",
    "        \n",
    "        # Start with existing data\n",
    "        if existing_decomm_columns:\n",
    "            decomm_df = monthly_df[existing_decomm_columns].copy()\n",
    "        else:\n",
    "            # Create empty dataframe with correct months\n",
    "            decomm_df = pd.DataFrame(index=all_months)\n",
    "        \n",
    "        # ✅ ADD MISSING DUIDs: Add columns for DUIDs that don't appear in monthly_df\n",
    "        for missing_duid in missing_decomm_duids:\n",
    "            decomm_df[missing_duid] = 0  # Fill with zeros since they had no data\n",
    "        \n",
    "        # Ensure all decommissioned DUIDs are now included\n",
    "        all_decomm_columns = list(decommissioned_duids)\n",
    "        decomm_df = decomm_df.reindex(columns=all_decomm_columns, fill_value=0)\n",
    "        \n",
    "        # Get metadata from the period where each DUID last appeared\n",
    "        decomm_metadata = {}\n",
    "        for duid in decommissioned_duids:\n",
    "            # Find latest period where this DUID appeared\n",
    "            for period_num in sorted(all_periods_metadata.keys(), reverse=True):\n",
    "                if duid in all_periods_metadata[period_num]:\n",
    "                    decomm_metadata[duid] = all_periods_metadata[period_num][duid]\n",
    "                    break\n",
    "            \n",
    "            # If no metadata found, create basic entry\n",
    "            if duid not in decomm_metadata:\n",
    "                decomm_metadata[duid] = {\n",
    "                    \"DUID\": duid,\n",
    "                    \"Name\": duid,\n",
    "                    \"Facility\": \"Unknown\",\n",
    "                    \"Region\": \"Unknown\", \n",
    "                    \"Fueltech\": \"Unknown\",\n",
    "                    \"Owner\": \"Unknown\",\n",
    "                    \"Number_of_Units\": \"N/A\",\n",
    "                    \"Nameplate_Capacity_MW\": \"N/A\",\n",
    "                    \"Storage_Capacity_MWh\": \"N/A\",\n",
    "                    \"Expected_Closure_Year\": \"N/A\"\n",
    "                }\n",
    "        \n",
    "        # Create decommissioned metadata rows\n",
    "        decomm_meta_rows = []\n",
    "        for field in meta_fields:\n",
    "            row = {}\n",
    "            for col in decomm_df.columns:\n",
    "                if col in decomm_metadata:\n",
    "                    value = decomm_metadata[col].get(field, \"N/A\")\n",
    "                    if pd.isna(value):\n",
    "                        value = \"N/A\"\n",
    "                    row[col] = value\n",
    "                else:\n",
    "                    row[col] = \"N/A\"\n",
    "            decomm_meta_rows.append(row)\n",
    "        \n",
    "        decomm_meta_df = pd.DataFrame(decomm_meta_rows, index=meta_fields)\n",
    "        decomm_separator = pd.DataFrame(index=[\"---\"], columns=decomm_df.columns)\n",
    "        decomm_final_df = pd.concat([decomm_meta_df, decomm_separator, decomm_df])\n",
    "        \n",
    "        # Save decommissioned file\n",
    "        decomm_filename = f\"{decommissioned_filename}.csv\"\n",
    "        decomm_final_df.to_csv(decomm_filename)\n",
    "        print(f\"✅ Decommissioned file saved: {decomm_filename}\")\n",
    "        print(f\"   • Total DUIDs included: {len(decomm_df.columns)}\")\n",
    "        print(f\"   • DUIDs with actual data: {len(existing_decomm_columns)}\")\n",
    "        print(f\"   • DUIDs with zero data (still historically present): {len(missing_decomm_duids)}\")\n",
    "        \n",
    "        # Show some examples of what was included\n",
    "        if missing_decomm_duids:\n",
    "            sample_missing = list(missing_decomm_duids)[:5]\n",
    "            print(f\"   • Sample zero-data DUIDs: {', '.join(sample_missing)}\")\n",
    "    else:\n",
    "        print(\"✅ No decommissioned DUIDs found\")\n",
    "\n",
    "# === MAIN LOGIC ===\n",
    "print(\"🔄 Loading NEM reference data...\")\n",
    "duid_lookup = load_nem_reference_data()\n",
    "\n",
    "# 🆕 NEW: Calculate periods (auto-handles API limit)\n",
    "periods = calculate_date_periods(start_month, start_year, end_month, end_year)\n",
    "\n",
    "# Display filename configuration\n",
    "print(f\"📁 Output filename: {consolidated_filename}.csv\")\n",
    "if len(periods) > 1:\n",
    "    print(f\"📁 Decommissioned filename: {decommissioned_filename}.csv\")\n",
    "print(\"⚡ Values will be in MWh (energy units)\")\n",
    "\n",
    "# Display filter settings\n",
    "if REGION_FILTER:\n",
    "    print(f\"🏞️ Region filter active: {', '.join(REGION_FILTER)}\")\n",
    "else:\n",
    "    print(\"🏞️ No region filter - including all regions\")\n",
    "\n",
    "print(\"🔄 Fetching facility codes...\")\n",
    "facility_codes = fetch_all_facility_codes(API_KEY)\n",
    "print(f\"✅ Retrieved {len(facility_codes)} facilities\")\n",
    "\n",
    "# 🆕 NEW: Fetch data for all periods with robust error handling\n",
    "all_records = []\n",
    "all_periods_metadata = {}\n",
    "\n",
    "for period_num, (start_date, end_date) in enumerate(periods, 1):\n",
    "    try:\n",
    "        records, metadata = fetch_data_for_period(facility_codes, METRIC, duid_lookup, start_date, end_date, period_num)\n",
    "        all_records.extend(records)\n",
    "        all_periods_metadata[period_num] = metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Period {period_num} failed with error: {str(e)}\")\n",
    "        print(f\"⚠️ Continuing with data from completed periods...\")\n",
    "        break\n",
    "\n",
    "if not all_records:\n",
    "    print(\"⚠️ No data returned.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n✅ Total data retrieved: {len(all_records)} records across {len(set(r['period'] for r in all_records))} periods\")\n",
    "\n",
    "if len(periods) == 1:\n",
    "    # Single period - create output like original code\n",
    "    df = pd.DataFrame(all_records)\n",
    "    metadata = all_periods_metadata[1]\n",
    "    \n",
    "    # 🧮 Add 'month' for grouping (same as your original code)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])  \n",
    "    df[\"month\"] = df[\"timestamp\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "    # 🗃️ Pivot into matrix format - aggregating by month (same as your original code)\n",
    "    print(\"🔄 Aggregating data by month...\")\n",
    "    monthly_df = df.groupby([\"month\", \"key\"])[\"value\"].sum().unstack(fill_value=0)\n",
    "\n",
    "    # 🔧 FIXED FILTERING LOGIC - Remove the is_matched restriction that was filtering out DUIDs\n",
    "    print(\"🔄 Filtering out N/A DUIDs...\")\n",
    "    valid_columns = []\n",
    "    for col in monthly_df.columns:\n",
    "        if col in metadata:\n",
    "            duid = metadata[col].get('DUID', 'N/A')\n",
    "            # ✅ FIXED: Only check if DUID is not N/A, don't require NEM match\n",
    "            # This allows DUIDs that only have API data to be included\n",
    "            if duid != 'N/A':\n",
    "                valid_columns.append(col)\n",
    "\n",
    "    monthly_df = monthly_df[valid_columns]\n",
    "    print(f\"✅ Filtered matrix: {monthly_df.shape[0]} months × {monthly_df.shape[1]} DUIDs\")\n",
    "\n",
    "    # Show region breakdown\n",
    "    if valid_columns:\n",
    "        region_counts = {}\n",
    "        storage_count = 0\n",
    "        for col in valid_columns:\n",
    "            if col in metadata:\n",
    "                region = metadata[col].get('Region', 'Unknown')\n",
    "                region_counts[region] = region_counts.get(region, 0) + 1\n",
    "                \n",
    "                # Count storage facilities\n",
    "                storage_capacity = metadata[col].get('Storage_Capacity_MWh', 'N/A')\n",
    "                if storage_capacity != 'N/A' and storage_capacity != '' and storage_capacity != 0:\n",
    "                    storage_count += 1\n",
    "        \n",
    "        print(f\"📊 Region breakdown:\")\n",
    "        for region, count in sorted(region_counts.items()):\n",
    "            print(f\"   • {region}: {count} DUIDs\")\n",
    "        \n",
    "        print(f\"🔋 Storage facilities in output: {storage_count}\")\n",
    "\n",
    "    # Alternative aggregation option (uncomment if you want averages instead of sums):\n",
    "    # monthly_df = df.groupby([\"month\", \"key\"])[\"value\"].mean().unstack(fill_value=0)\n",
    "\n",
    "    # 🏷️ Add enhanced metadata as header rows (removed Metric field)\n",
    "    # Check if we have enhanced data\n",
    "    has_enhanced_data = any('Owner' in meta for meta in metadata.values())\n",
    "\n",
    "    if has_enhanced_data:\n",
    "        meta_fields = [\"DUID\", \"Name\", \"Facility\", \"Region\", \"Fueltech\", \n",
    "                       \"Owner\", \"Number_of_Units\", \"Nameplate_Capacity_MW\", \n",
    "                       \"Storage_Capacity_MWh\", \"Expected_Closure_Year\"]\n",
    "    else:\n",
    "        meta_fields = [\"DUID\", \"Name\", \"Facility\", \"Region\", \"Fueltech\"]\n",
    "\n",
    "    meta_rows = []\n",
    "    for field in meta_fields:\n",
    "        row = {}\n",
    "        for col in monthly_df.columns:\n",
    "            if col in metadata:\n",
    "                value = metadata[col].get(field, \"N/A\")\n",
    "                # ✅ ADDITIONAL FIX: Handle pandas NaN values that might cause issues\n",
    "                if pd.isna(value):\n",
    "                    value = \"N/A\"\n",
    "                row[col] = value\n",
    "            else:\n",
    "                row[col] = \"N/A\"\n",
    "        meta_rows.append(row)\n",
    "\n",
    "    meta_df = pd.DataFrame(meta_rows, index=meta_fields)\n",
    "    separator = pd.DataFrame(index=[\"---\"], columns=monthly_df.columns)\n",
    "    final_df = pd.concat([meta_df, separator, monthly_df])\n",
    "\n",
    "    # 💾 Save file with configurable name\n",
    "    filename = f\"{consolidated_filename}.csv\"\n",
    "    final_df.to_csv(filename)\n",
    "    print(f\"\\n✅ Enhanced file saved: {filename}\")\n",
    "\n",
    "    # 📊 Generate summary report with storage info\n",
    "    matched_duids = len([key for key in metadata.keys() if key in duid_lookup])\n",
    "    total_duids = len(metadata)\n",
    "    filtered_duids = len(valid_columns)\n",
    "\n",
    "    # Count storage facilities\n",
    "    storage_duids_with_data = 0\n",
    "    for key in valid_columns:\n",
    "        if key in metadata:\n",
    "            storage_capacity = metadata[key].get('Storage_Capacity_MWh', 'N/A')\n",
    "            if storage_capacity != 'N/A' and storage_capacity != '' and storage_capacity != 0:\n",
    "                storage_duids_with_data += 1\n",
    "\n",
    "    print(f\"\\n📊 SUMMARY REPORT:\")\n",
    "    print(f\"   • File saved as: {filename}\")\n",
    "    print(f\"   • Values in MWh (energy units)\")\n",
    "    print(f\"   • Region filter: {', '.join(REGION_FILTER) if REGION_FILTER else 'None (all regions)'}\")\n",
    "    print(f\"   • Total DUIDs from API: {total_duids}\")\n",
    "    print(f\"   • DUIDs matched with NEM data: {matched_duids}\")\n",
    "    print(f\"   • DUIDs included in final output: {filtered_duids}\")\n",
    "    print(f\"   • Match rate: {(matched_duids/total_duids*100):.1f}%\" if total_duids > 0 else \"   • Match rate: 0%\")\n",
    "    print(f\"   • NEM reference data loaded: {len(duid_lookup)} DUIDs\")\n",
    "\n",
    "else:\n",
    "    # Multiple periods - use new logic\n",
    "    # 🆕 NEW: Categorize DUIDs\n",
    "    reference_duids, decommissioned_duids, latest_period = categorize_duids(all_periods_metadata)\n",
    "\n",
    "    # 🆕 NEW: Create output files\n",
    "    create_output_files(all_records, all_periods_metadata, reference_duids, decommissioned_duids, latest_period)\n",
    "\n",
    "    # ✅ ENHANCED SUMMARY REPORTING\n",
    "    matched_duids = len([key for period_meta in all_periods_metadata.values() for key in period_meta.keys() if key in duid_lookup])\n",
    "    total_duids = len(set(key for period_meta in all_periods_metadata.values() for key in period_meta.keys()))\n",
    "    \n",
    "    # Count how many decommissioned DUIDs actually have data\n",
    "    df = pd.DataFrame(all_records)\n",
    "    if not df.empty:\n",
    "        df[\"month\"] = pd.to_datetime(df[\"timestamp\"]).dt.to_period(\"M\").astype(str)\n",
    "        monthly_df = df.groupby([\"month\", \"key\"])[\"value\"].sum().unstack(fill_value=0)\n",
    "        decomm_with_data = len([col for col in monthly_df.columns if col in decommissioned_duids])\n",
    "    else:\n",
    "        decomm_with_data = 0\n",
    "\n",
    "    print(f\"\\n📊 ENHANCED SUMMARY REPORT:\")\n",
    "    print(f\"   • Files saved as: {consolidated_filename}.csv, {decommissioned_filename}.csv\")\n",
    "    print(f\"   • Values in MWh (energy units)\")\n",
    "    print(f\"   • Date range processed: {periods[0][0].strftime('%Y-%m-%d')} to {periods[-1][1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   • Periods processed: {len(set(r['period'] for r in all_records))}/{len(periods)}\")\n",
    "    print(f\"   • Region filter: {', '.join(REGION_FILTER) if REGION_FILTER else 'None (all regions)'}\")\n",
    "    print(f\"   • Reference DUIDs (main file): {len(reference_duids)}\")\n",
    "    print(f\"   • Decommissioned DUIDs identified: {len(decommissioned_duids)}\")\n",
    "    print(f\"   • Decommissioned DUIDs with substantial data: {decomm_with_data}\")\n",
    "    print(f\"   • Decommissioned DUIDs with minimal/zero data: {len(decommissioned_duids) - decomm_with_data}\")\n",
    "    print(f\"   • Total unique DUIDs across all periods: {total_duids}\")\n",
    "    print(f\"   • NEM reference data loaded: {len(duid_lookup)} DUIDs\")\n",
    "    print(f\"\")\n",
    "    print(f\"🔍 DECOMMISSIONED TRACKING:\")\n",
    "    print(f\"   • ALL {len(decommissioned_duids)} historically present DUIDs included in decommissioned file\")\n",
    "    print(f\"   • This includes facilities that may have minimal generation data\")\n",
    "    print(f\"   • Useful for tracking grid evolution and facility closure patterns\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33f79994913ccb4c81c421a51466dbe971696a8bea0839e8a0b59247fbe24294"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
